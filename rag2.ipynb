{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded passages: torch.Size([150, 768])\n"
     ]
    }
   ],
   "source": [
    "# og embedding with just one document\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the retriever model\n",
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load data from the CSV file\n",
    "df = pd.read_csv(\"./151_ideas_updated.csv\", usecols=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Ensure the correct column is used for passages\n",
    "df.columns = df.columns.str.strip()  # Remove any extra spaces in column names\n",
    "passages = df[\"Ideas\"].dropna().tolist()  # Replace \"Ideas\" with the actual column name if different\n",
    "\n",
    "# Encode the passages\n",
    "passage_embeddings = retriever_model.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Encoded passages:\", passage_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Ideas    Theme a    Theme-b  \\\n",
      "0  1) Maximize the Beauty - fully channel the bea...        fun   rational   \n",
      "1  2) Full Expression - it takes a lot of effort ...  inspiring  intuitive   \n",
      "2  3) Expect Rising - this means our expectations...   rational   negative   \n",
      "3  4) The Power of Pettiness - is the idea that p...      chill  inspiring   \n",
      "4  5) Various meditation - I am a big fan of medi...      chill        fun   \n",
      "\n",
      "     Theme-c Unnamed: 4                                         Unnamed: 5  \\\n",
      "0   positive   personal  theme ideas - rough, rational, intuitive, posi...   \n",
      "1   positive   personal  top 5 idea themes- rational, positive, inspiri...   \n",
      "2   negative        NaN                                                NaN   \n",
      "3   positive        NaN                                                NaN   \n",
      "4  inspiring        NaN                                                NaN   \n",
      "\n",
      "  Question Answer  \n",
      "0      NaN    NaN  \n",
      "1      NaN    NaN  \n",
      "2      NaN    NaN  \n",
      "3      NaN    NaN  \n",
      "4      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# embedding with 2 documents\n",
    "import pandas as pd\n",
    "\n",
    "# Load the existing structured CSV\n",
    "structured_df = pd.read_csv(\"./151_ideas_updated.csv\", usecols=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Load the less structured CSV\n",
    "less_structured_df = pd.read_csv('./151qa2.csv', usecols=['text'])\n",
    "\n",
    "# Normalize text to lowercase for consistent processing\n",
    "less_structured_df['text'] = less_structured_df['text'].str.lower()\n",
    "\n",
    "# Initialize lists to hold processed questions and answers\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# Temporary storage for current question and answer\n",
    "current_question = None\n",
    "current_answer = None\n",
    "\n",
    "# Iterate through the rows\n",
    "for _, row in less_structured_df.iterrows():\n",
    "    text = row['text'].strip()\n",
    "    if text.startswith('q:'):\n",
    "        # If there's a current question, finalize it without an answer\n",
    "        if current_question:\n",
    "            questions.append(current_question)\n",
    "            answers.append(None)\n",
    "        # Start a new question\n",
    "        current_question = text[2:].strip()\n",
    "        current_answer = None\n",
    "    elif text.startswith('a:'):\n",
    "        # If there's an answer, associate it with the current question\n",
    "        current_answer = text[2:].strip()\n",
    "        if current_question:  # Only add if there is a question\n",
    "            questions.append(current_question)\n",
    "            answers.append(current_answer)\n",
    "            current_question = None\n",
    "            current_answer = None\n",
    "    else:\n",
    "        # Ignore rows that are neither questions nor answers\n",
    "        pass\n",
    "\n",
    "# Handle any trailing question without an answer\n",
    "if current_question:\n",
    "    questions.append(current_question)\n",
    "    answers.append(None)\n",
    "\n",
    "# Combine questions and answers into a structured DataFrame\n",
    "new_data = pd.DataFrame({\n",
    "    'Question': questions,\n",
    "    'Answer': answers\n",
    "})\n",
    "\n",
    "# Add placeholder columns to match the structure of the existing DataFrame\n",
    "for col in structured_df.columns:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None  # Fill with None or another placeholder value\n",
    "\n",
    "# Append the new data to the existing structured DataFrame\n",
    "updated_df = pd.concat([structured_df, new_data], ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV\n",
    "updated_df.to_csv(\"./updated_ideas.csv\", index=False)\n",
    "\n",
    "# Inspect the first few rows of the updated DataFrame\n",
    "print(updated_df.head())\n",
    "# Ensure the correct column is used for passages\n",
    "df.columns = df.columns.str.strip()  # Remove any extra spaces in column names\n",
    "passages = df[\"Ideas\"].dropna().tolist()  # Replace \"Ideas\" with the actual column name if different\n",
    "\n",
    "# Encode the passages\n",
    "passage_embeddings = retriever_model.encode(passages, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Passages: 100%|██████████| 9/9 [00:09<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BAAI/bge-base-en-v1.5\n",
      "Total Passages: 271\n",
      "Embedding Shape: torch.Size([271, 768])\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_encode_passages(csv_path, text_columns, model_name=\"BAAI/bge-base-en-v1.5\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Optimized function to load passages and create embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to the CSV file\n",
    "    - text_columns: List of column names to combine for embeddings\n",
    "    - model_name: Sentence transformer model to use\n",
    "    - batch_size: Number of passages to encode in each batch\n",
    "    \n",
    "    Returns:\n",
    "    - passages: List of cleaned passages\n",
    "    - passage_embeddings: Tensor of embeddings\n",
    "    \"\"\"\n",
    "    # Load data with error handling\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine specified text columns into a single passage\n",
    "    df['combined_text'] = df[text_columns].fillna('').agg(' '.join, axis=1).str.strip()\n",
    "    passages = df['combined_text'].dropna().tolist()\n",
    "    \n",
    "    # Check if passages are empty\n",
    "    if not passages:\n",
    "        print(\"No valid passages found in the specified columns.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize model with device optimization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    retriever_model = SentenceTransformer(model_name).to(device)\n",
    "    \n",
    "    # Encode passages in batches with progress tracking\n",
    "    passage_embeddings = []\n",
    "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding Passages\"):\n",
    "        batch = passages[i:i+batch_size]\n",
    "        batch_embeddings = retriever_model.encode(\n",
    "            batch, \n",
    "            convert_to_tensor=True, \n",
    "            device=device, \n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        passage_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Combine batch embeddings\n",
    "    passage_embeddings = torch.cat(passage_embeddings, dim=0)\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total Passages: {len(passages)}\")\n",
    "    print(f\"Embedding Shape: {passage_embeddings.shape}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return passages, passage_embeddings\n",
    "\n",
    "# Usage\n",
    "csv_path = \"updated_ideas.csv\"\n",
    "passages, embeddings = load_and_encode_passages(\n",
    "    csv_path, \n",
    "    text_columns=[\"Question\", \"Answer\"],  # Specify the columns to combine\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\", \n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Passages: 100%|██████████| 5/5 [00:31<00:00,  6.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BAAI/bge-base-en-v1.5\n",
      "Total Passages: 150\n",
      "Embedding Shape: torch.Size([150, 768])\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# metrics - benchmarks etc  embedding with 2 documents\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_encode_passages(csv_path, text_column, model_name=\"BAAI/bge-base-en-v1.5\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Optimized function to load passages and create embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to the CSV file\n",
    "    - text_column: Name of the column containing passages\n",
    "    - model_name: Sentence transformer model to use\n",
    "    - batch_size: Number of passages to encode in each batch\n",
    "    \n",
    "    Returns:\n",
    "    - passages: List of cleaned passages\n",
    "    - passage_embeddings: Tensor of embeddings\n",
    "    \"\"\"\n",
    "    # Load data with error handling\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
    "        df.columns = df.columns.str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Clean and filter passages\n",
    "    passages = df[text_column].dropna().tolist()\n",
    "    \n",
    "    # Check if passages are empty\n",
    "    if not passages:\n",
    "        print(\"No valid passages found in the specified column.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize model with device optimization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    retriever_model = SentenceTransformer(model_name).to(device)\n",
    "    \n",
    "    # Encode passages in batches with progress tracking\n",
    "    passage_embeddings = []\n",
    "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding Passages\"):\n",
    "        batch = passages[i:i+batch_size]\n",
    "        batch_embeddings = retriever_model.encode(\n",
    "            batch, \n",
    "            convert_to_tensor=True, \n",
    "            device=device, \n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        passage_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Combine batch embeddings\n",
    "    passage_embeddings = torch.cat(passage_embeddings, dim=0)\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total Passages: {len(passages)}\")\n",
    "    print(f\"Embedding Shape: {passage_embeddings.shape}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return passages, passage_embeddings\n",
    "\n",
    "# Usage\n",
    "csv_path = \"151_ideas_updated.csv\"\n",
    "passages, embeddings = load_and_encode_passages(\n",
    "    csv_path, \n",
    "    text_column=\"Ideas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Passages: 100%|██████████| 5/5 [00:26<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BAAI/bge-base-en-v1.5\n",
      "Total Passages: 150\n",
      "Embedding Shape: torch.Size([150, 768])\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics - benchmarks etc\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_encode_passages(csv_path, text_column, model_name=\"BAAI/bge-base-en-v1.5\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Optimized function to load passages and create embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to the CSV file\n",
    "    - text_column: Name of the column containing passages\n",
    "    - model_name: Sentence transformer model to use\n",
    "    - batch_size: Number of passages to encode in each batch\n",
    "    \n",
    "    Returns:\n",
    "    - passages: List of cleaned passages\n",
    "    - passage_embeddings: Tensor of embeddings\n",
    "    \"\"\"\n",
    "    # Load data with error handling\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
    "        df.columns = df.columns.str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Clean and filter passages\n",
    "    passages = df[text_column].dropna().tolist()\n",
    "    \n",
    "    # Check if passages are empty\n",
    "    if not passages:\n",
    "        print(\"No valid passages found in the specified column.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize model with device optimization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    retriever_model = SentenceTransformer(model_name).to(device)\n",
    "    \n",
    "    # Encode passages in batches with progress tracking\n",
    "    passage_embeddings = []\n",
    "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding Passages\"):\n",
    "        batch = passages[i:i+batch_size]\n",
    "        batch_embeddings = retriever_model.encode(\n",
    "            batch, \n",
    "            convert_to_tensor=True, \n",
    "            device=device, \n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        passage_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Combine batch embeddings\n",
    "    passage_embeddings = torch.cat(passage_embeddings, dim=0)\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total Passages: {len(passages)}\")\n",
    "    print(f\"Embedding Shape: {passage_embeddings.shape}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return passages, passage_embeddings\n",
    "\n",
    "# Usage\n",
    "csv_path = \"151_ideas_updated.csv\"\n",
    "passages, embeddings = load_and_encode_passages(\n",
    "    csv_path, \n",
    "    text_column=\"Ideas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'passage_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure the embeddings are converted to a NumPy array\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m passage_embeddings_np \u001b[38;5;241m=\u001b[39m \u001b[43mpassage_embeddings\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convert tensor to NumPy array\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a FAISS index\u001b[39;00m\n\u001b[1;32m      8\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(passage_embeddings_np\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Dimensionality of the embeddings\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'passage_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the embeddings are converted to a NumPy array\n",
    "passage_embeddings_np = passage_embeddings.cpu().numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(passage_embeddings_np.shape[1])  # Dimensionality of the embeddings\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(passage_embeddings_np)\n",
    "\n",
    "print(\"Number of embeddings in index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     53\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe nature of human consciousness and technological existence\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 54\u001b[0m philosophical_response \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_and_generate_philosophical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(philosophical_response)\n",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m, in \u001b[0;36mretrieve_and_generate_philosophical\u001b[0;34m(query, top_k, min_tokens, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_and_generate_philosophical\u001b[39m(query, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Retrieve embeddings\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m retriever_model\u001b[38;5;241m.\u001b[39mencode([query], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 4\u001b[0m     _, indices \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39msearch(query_embedding, top_k)\n\u001b[1;32m      5\u001b[0m     retrieved_passages \u001b[38;5;241m=\u001b[39m [passages[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Philosophical prompt construction\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "def retrieve_and_generate_philosophical(query, top_k=5, min_tokens=700, max_length=2000):\n",
    "    # Retrieve embeddings\n",
    "    query_embedding = retriever_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_passages = [passages[i] for i in indices[0]]\n",
    "    \n",
    "    # Philosophical prompt construction\n",
    "    philosophical_prompt = f\"\"\"Philosophical Meditation on: \"{query}\"\n",
    "\n",
    "Contextual Fragments of Existence:\n",
    "{' '.join(retrieved_passages)}\n",
    "\n",
    "Phenomenological Exploration Directive:\n",
    "\n",
    "Engage in a profound philosophical discourse that transcends mere information retrieval. Your response should:\n",
    "\n",
    "- Interrogate the ontological foundations of the retrieved knowledge\n",
    "- Deconstruct epistemological assumptions inherent in the passages\n",
    "- Explore the liminal spaces between understanding and mystery\n",
    "- Invoke philosophical traditions: existentialism, phenomenology, hermeneutics\n",
    "- Reveal the metaphysical undercurrents beneath empirical observations\n",
    "- Challenge the boundaries of human comprehension\n",
    "- Illuminate the inherent paradoxes of knowledge and perception\n",
    "\n",
    "Philosophical Meditation Guidelines:\n",
    "- Embrace uncertainty as a mode of understanding\n",
    "- Treat each fragment as a gateway to deeper existential inquiry\n",
    "- Resist the temptation of definitive conclusions\n",
    "- Navigate the terrain between rationality and the ineffable\n",
    "- Unveil the poetic subtext of intellectual exploration\n",
    "\n",
    "Synthesize a profound philosophical reflection that transforms retrieved information into a meditation on human experience, consciousness, and the sublime:\"\"\"\n",
    "    \n",
    "    # Generation with enhanced philosophical parameters\n",
    "    input_ids = generator_tokenizer.encode(philosophical_prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = generator_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        min_length=min_tokens,\n",
    "        temperature=0.85,  # Higher creativity for philosophical exploration\n",
    "        do_sample=True,\n",
    "        top_k=120,  # Broader conceptual sampling\n",
    "        top_p=0.96,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.7,  # Encourage expansive philosophical discourse\n",
    "        repetition_penalty=1.3  # Reduce repetitive philosophical language\n",
    "    )\n",
    "    \n",
    "    return generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "query = \"the nature of human consciousness and technological existence\"\n",
    "philosophical_response = retrieve_and_generate_philosophical(query)\n",
    "print(philosophical_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:657: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.2` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a feminist, but not enough to be feminist. I see half the world suffering from cynicism that is stunting civilization. gitty up! Update 2-19-19 Feminism is important to me because I've seen half of the population suffering... but if that’s not sufficient to have feminism, I don’t know what is. Prejudice is inherently dumb. If you see something say something, right? We see its extreme importance in all of our lives. We saw it».'....................)??.............................................? fa  ne -', just., 'MtB – slut..,.“””..: Patriarchy?...? (.t:..!...and d'' ::)? and! e.g. when i, are &?; ;. (M.W.B. more than one, and have the p.- t,.&. in mtb, this ’m:-;&& o. and how l. — and for...........–...no,, it,...to... and even, at yo...,........... h.s. the. [ed.;]:(...) the beauty.â») as tru.o;; or, we see:?!.» () in every.’: [)-. but. that. just r.a. or w. on this:,;)..C.self.T.A..com;-;...the.p;, as. all. people.i.P.E. of.\"B\" v..S.M:E in our hearts.R.• and to. \".>. »\" in: \"D.f.m. is...d? as, in.D; in.#!;?(): at. about;\":;m;e) all our,\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_generate(query, top_k=5, min_tokens=500, max_length=1500):\n",
    "    # Encode the query\n",
    "    query_embedding = retriever_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # Retrieve top_k passages\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_passages = [passages[i] for i in indices[0]]\n",
    "\n",
    "    # Combine retrieved passages\n",
    "    input_text = f\"{query} {' '.join(retrieved_passages)} Discuss failures, beauty, and the certainty of death with examples.\"\n",
    "\n",
    "    # Generate response\n",
    "    input_ids = generator_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = generator_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        min_length=min_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.2  # Encourage longer responses\n",
    "    )\n",
    "\n",
    "    return generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "query = \"list points that mention neech\"\n",
    "print(retrieve_and_generate(query, min_tokens=500, max_length=1000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
