{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded passages: torch.Size([150, 768])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the retriever model\n",
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load data from the CSV file\n",
    "df = pd.read_csv(\"./151_ideas_updated.csv\", usecols=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Ensure the correct column is used for passages\n",
    "df.columns = df.columns.str.strip()  # Remove any extra spaces in column names\n",
    "passages = df[\"Ideas\"].dropna().tolist()  # Replace \"Ideas\" with the actual column name if different\n",
    "\n",
    "# Encode the passages\n",
    "passage_embeddings = retriever_model.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Encoded passages:\", passage_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Passages: 100%|██████████| 5/5 [00:28<00:00,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BAAI/bge-base-en-v1.5\n",
      "Total Passages: 150\n",
      "Embedding Shape: torch.Size([150, 768])\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics - benchmarks etc\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_encode_passages(csv_path, text_column, model_name=\"BAAI/bge-base-en-v1.5\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Optimized function to load passages and create embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path: Path to the CSV file\n",
    "    - text_column: Name of the column containing passages\n",
    "    - model_name: Sentence transformer model to use\n",
    "    - batch_size: Number of passages to encode in each batch\n",
    "    \n",
    "    Returns:\n",
    "    - passages: List of cleaned passages\n",
    "    - passage_embeddings: Tensor of embeddings\n",
    "    \"\"\"\n",
    "    # Load data with error handling\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
    "        df.columns = df.columns.str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Clean and filter passages\n",
    "    passages = df[text_column].dropna().tolist()\n",
    "    \n",
    "    # Check if passages are empty\n",
    "    if not passages:\n",
    "        print(\"No valid passages found in the specified column.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize model with device optimization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    retriever_model = SentenceTransformer(model_name).to(device)\n",
    "    \n",
    "    # Encode passages in batches with progress tracking\n",
    "    passage_embeddings = []\n",
    "    for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding Passages\"):\n",
    "        batch = passages[i:i+batch_size]\n",
    "        batch_embeddings = retriever_model.encode(\n",
    "            batch, \n",
    "            convert_to_tensor=True, \n",
    "            device=device, \n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        passage_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Combine batch embeddings\n",
    "    passage_embeddings = torch.cat(passage_embeddings, dim=0)\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total Passages: {len(passages)}\")\n",
    "    print(f\"Embedding Shape: {passage_embeddings.shape}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return passages, passage_embeddings\n",
    "\n",
    "# Usage\n",
    "csv_path = \"151_ideas_updated.csv\"\n",
    "passages, embeddings = load_and_encode_passages(\n",
    "    csv_path, \n",
    "    text_column=\"Ideas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in index: 150\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the embeddings are converted to a NumPy array\n",
    "passage_embeddings_np = passage_embeddings.cpu().numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(passage_embeddings_np.shape[1])  # Dimensionality of the embeddings\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(passage_embeddings_np)\n",
    "\n",
    "print(\"Number of embeddings in index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:657: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.2` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a feminist, but not enough to be feminist. I see half the world suffering from cynicism that is stunting civilization. gitty up! Update 2-19-19 Feminism is important to me because I've seen half of the population suffering... but if that’s not sufficient to have feminism, I don’t know what is. Prejudice is inherently dumb. If you see something say something, right? We see its extreme importance in all of our lives. We saw it».'....................)??.............................................? fa  ne -', just., 'MtB – slut..,.“””..: Patriarchy?...? (.t:..!...and d'' ::)? and! e.g. when i, are &?; ;. (M.W.B. more than one, and have the p.- t,.&. in mtb, this ’m:-;&& o. and how l. — and for...........–...no,, it,...to... and even, at yo...,........... h.s. the. [ed.;]:(...) the beauty.â») as tru.o;; or, we see:?!.» () in every.’: [)-. but. that. just r.a. or w. on this:,;)..C.self.T.A..com;-;...the.p;, as. all. people.i.P.E. of.\"B\" v..S.M:E in our hearts.R.• and to. \".>. »\" in: \"D.f.m. is...d? as, in.D; in.#!;?(): at. about;\":;m;e) all our,\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_generate(query, top_k=5, min_tokens=500, max_length=1500):\n",
    "    # Encode the query\n",
    "    query_embedding = retriever_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # Retrieve top_k passages\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_passages = [passages[i] for i in indices[0]]\n",
    "\n",
    "    # Combine retrieved passages\n",
    "    input_text = f\"{query} {' '.join(retrieved_passages)} Discuss failures, beauty, and the certainty of death with examples.\"\n",
    "\n",
    "    # Generate response\n",
    "    input_ids = generator_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = generator_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        min_length=min_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.2  # Encourage longer responses\n",
    "    )\n",
    "\n",
    "    return generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "query = \"list points that mention neech\"\n",
    "print(retrieve_and_generate(query, min_tokens=500, max_length=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
