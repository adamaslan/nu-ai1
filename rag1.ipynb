{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded passages: torch.Size([150, 768])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the retriever model\n",
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load data from the CSV file\n",
    "df = pd.read_csv(\"151_ideas_updated.csv\")  # Replace with the correct file path\n",
    "\n",
    "# Ensure the correct column is used for passages\n",
    "df.columns = df.columns.str.strip()  # Remove any extra spaces in column names\n",
    "passages = df[\"Ideas\"].dropna().tolist()  # Replace \"Ideas\" with the actual column name if different\n",
    "\n",
    "# Encode the passages\n",
    "passage_embeddings = retriever_model.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Encoded passages:\", passage_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in index: 150\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the embeddings are converted to a NumPy array\n",
    "passage_embeddings_np = passage_embeddings.cpu().numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(passage_embeddings_np.shape[1])  # Dimensionality of the embeddings\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(passage_embeddings_np)\n",
    "\n",
    "print(\"Number of embeddings in index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:657: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.2` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a feminist, but not enough to be feminist. I see half the world suffering from cynicism that is stunting civilization. gitty up! Update 2-19-19 Feminism is important to me because I've seen half of the population suffering... but if that’s not sufficient to have feminism, I don’t know what is. Prejudice is inherently dumb. If you see something say something, right? We see its extreme importance in all of our lives. We saw it».'....................)??.............................................? fa  ne -', just., 'MtB – slut..,.“””..: Patriarchy?...? (.t:..!...and d'' ::)? and! e.g. when i, are &?; ;. (M.W.B. more than one, and have the p.- t,.&. in mtb, this ’m:-;&& o. and how l. — and for...........–...no,, it,...to... and even, at yo...,........... h.s. the. [ed.;]:(...) the beauty.â») as tru.o;; or, we see:?!.» () in every.’: [)-. but. that. just r.a. or w. on this:,;)..C.self.T.A..com;-;...the.p;, as. all. people.i.P.E. of.\"B\" v..S.M:E in our hearts.R.• and to. \".>. »\" in: \"D.f.m. is...d? as, in.D; in.#!;?(): at. about;\":;m;e) all our,\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_generate(query, top_k=5, min_tokens=500, max_length=1500):\n",
    "    # Encode the query\n",
    "    query_embedding = retriever_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # Retrieve top_k passages\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_passages = [passages[i] for i in indices[0]]\n",
    "\n",
    "    # Combine retrieved passages\n",
    "    input_text = f\"{query} {' '.join(retrieved_passages)} Discuss failures, beauty, and the certainty of death with examples.\"\n",
    "\n",
    "    # Generate response\n",
    "    input_ids = generator_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = generator_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        min_length=min_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.2  # Encourage longer responses\n",
    "    )\n",
    "\n",
    "    return generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "query = \"list points that mention beauty\"\n",
    "print(retrieve_and_generate(query, min_tokens=500, max_length=1000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
