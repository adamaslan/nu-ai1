{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# flux attempt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Load the base model\n",
    "base_model_name = \"enhanceaiteam/Flux-Uncensored-V2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Use `mps` for Apple Silicon GPUs or CPU as a fallback\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16 if torch.has_mps else torch.float32).to(device)\n",
    "\n",
    "# Load the uncensored LoRA weights\n",
    "lora_weights_path = \"enhanceaiteam/Flux-uncensored-v2\"\n",
    "config = PeftConfig.from_pretrained(lora_weights_path)\n",
    "model = PeftModel.from_pretrained(model, lora_weights_path)\n",
    "\n",
    "# Example usage with an uncensored prompt\n",
    "prompt = \"A story about a forbidden love in a mystical forest.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
